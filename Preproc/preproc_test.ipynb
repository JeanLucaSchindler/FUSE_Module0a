{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PyPDF2 import PdfReader\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(string):\n",
    "    cleaned_s = re.sub(r'[\\x00-\\x1F\\x7F]', '', string)\n",
    "    return cleaned_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_garbled(text):\n",
    "    \"\"\"Detects garbled text output from PDFs.\"\"\"\n",
    "    # Check for (cid:xxx) patterns\n",
    "    if re.search(r\"\\(cid:\\d+\\)\", text):\n",
    "        return True\n",
    "\n",
    "    # Check for excessive non-ASCII characters\n",
    "    non_ascii_ratio = sum(1 for char in text if ord(char) > 127) / max(len(text), 1)\n",
    "    if non_ascii_ratio > 0.3:  # Adjust threshold as needed\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glued_text(text, long_word_threshold=30, glue_ratio_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Detects if a text has excessive glued words (missing spaces).\n",
    "\n",
    "    Args:\n",
    "        text (str): The extracted text.\n",
    "        long_word_threshold (int): Word length above which words are considered suspicious.\n",
    "        glue_ratio_threshold (float): Threshold ratio of suspicious words for flagging text.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if text likely contains glued words, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    words = text.split()\n",
    "    total_words = len(words)\n",
    "\n",
    "    # If there's no text or no words, we can't meaningfully flag it\n",
    "    if total_words == 0:\n",
    "        return False\n",
    "\n",
    "    suspicious_words = []\n",
    "    for w in words:\n",
    "        # 1) Very long words\n",
    "        if len(w) >= long_word_threshold:\n",
    "            suspicious_words.append(w)\n",
    "            continue\n",
    "\n",
    "        # 2) Lowercase-to-Uppercase transition inside the word\n",
    "        if re.search(r'[a-z][A-Z]', w):\n",
    "            suspicious_words.append(w)\n",
    "            continue\n",
    "\n",
    "        # 3) Punctuation immediately followed by a letter (no space)\n",
    "        if re.search(r'[.,;:!?][A-Za-z]', w):\n",
    "            suspicious_words.append(w)\n",
    "            continue\n",
    "\n",
    "    # Ratio of suspicious words to total words\n",
    "    suspicious_ratio = len(suspicious_words) / total_words\n",
    "\n",
    "    return suspicious_ratio > glue_ratio_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spaced_text(text, short_word_threshold=1, spacing_ratio_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Detects if a text likely has excessive spacing (e.g., a space between every character).\n",
    "\n",
    "    Args:\n",
    "        text (str): The extracted text.\n",
    "        short_word_threshold (int): Word length at or below which words are considered suspiciously short.\n",
    "        spacing_ratio_threshold (float): Threshold ratio of suspiciously short words for flagging text.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if text likely contains excessive spacing, False otherwise.\n",
    "    \"\"\"\n",
    "    # Split text on whitespace to get words\n",
    "    words = text.split()\n",
    "    total_words = len(words)\n",
    "\n",
    "    # If there are no words, we can't do much analysis\n",
    "    if total_words == 0:\n",
    "        return False\n",
    "\n",
    "    # Find all words at or below the short_word_threshold (e.g., length <= 1)\n",
    "    short_words = [w for w in words if len(w) <= short_word_threshold]\n",
    "\n",
    "    # Ratio of short words to total words\n",
    "    short_words_ratio = len(short_words) / total_words\n",
    "\n",
    "    # If the ratio of short (single-char) words is above the specified threshold,\n",
    "    # consider the text \"excessively spaced\"\n",
    "    if short_words_ratio > spacing_ratio_threshold:\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_pages(text, min_words=400):\n",
    "    \"\"\"Splits text into pages at paragraph breaks if they exceed min_words.\"\"\"\n",
    "    pages = []\n",
    "    current_page_lines = []\n",
    "    current_word_count = 0\n",
    "    threshold_reached = False\n",
    "\n",
    "    for line in text.splitlines():\n",
    "        current_page_lines.append(line)\n",
    "        current_word_count += len(line.split())\n",
    "\n",
    "        if current_word_count >= min_words:\n",
    "            threshold_reached = True\n",
    "\n",
    "        if threshold_reached and not line.strip():\n",
    "            page_text = \" \".join(current_page_lines).strip()\n",
    "            page_text = clean_text(page_text)\n",
    "            if page_text:\n",
    "                pages.append(page_text)\n",
    "            current_page_lines = []\n",
    "            current_word_count = 0\n",
    "            threshold_reached = False\n",
    "\n",
    "    if current_page_lines:\n",
    "        page_text = \"\".join(current_page_lines).strip()\n",
    "        if page_text:\n",
    "            pages.append(page_text)\n",
    "\n",
    "    return pages\n",
    "\n",
    "def extract_pdf_text(file_path):\n",
    "    \"\"\"Extracts text from a PDF page-by-page.\"\"\"\n",
    "\n",
    "    text_pages = []\n",
    "\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        reader = PdfReader(file)\n",
    "        all_text_empty = True\n",
    "        garbage = False\n",
    "        glued = False\n",
    "\n",
    "        for page in reader.pages:\n",
    "            text = page.extract_text()\n",
    "            text = clean_text(text)\n",
    "\n",
    "            if spaced_text(text):\n",
    "                text = text.replace(\" \", \"\")\n",
    "            if spaced_text(text):\n",
    "                text = text.replace(\" \", \"\")\n",
    "\n",
    "            glued =  glued_text(text)\n",
    "            garbage = is_garbled(text)\n",
    "\n",
    "            if garbage or glued:\n",
    "                break\n",
    "            if text and text.strip():\n",
    "                all_text_empty = False\n",
    "                text_pages.append(text.strip())\n",
    "            else:\n",
    "                text_pages.append(\"\")  # placeholder for now\n",
    "\n",
    "    return text_pages, all_text_empty, garbage, glued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_info(filename):\n",
    "    try:\n",
    "        filename_list = filename.split('_')\n",
    "        author = filename_list[0]\n",
    "        title = filename_list[1]\n",
    "        year = filename_list[2]\n",
    "\n",
    "        return author, title, year\n",
    "    except:\n",
    "        return \"unknown\", filename, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_books_dict(folder_path, output_json_path, min_words=400):\n",
    "    \"\"\"\n",
    "    Processes a folder of files and splits them into pages, handling:\n",
    "      - .txt files with paragraph splitting\n",
    "      - .pdf files (detecting digital text or applying OCR for scanned/printed PDFs)\n",
    "      - Skips unsupported files\n",
    "\n",
    "    Outputs a dictionary:\n",
    "    {\n",
    "        \"filename\": {\n",
    "            \"author\": \"default_author\",\n",
    "            \"pages\": [...]\n",
    "        },\n",
    "        ...\n",
    "    }\n",
    "\n",
    "    Also saves this dictionary to a JSON file.\n",
    "    \"\"\"\n",
    "    compiled_books = {}\n",
    "\n",
    "    ocr_pile = []\n",
    "    error_pile = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        print()\n",
    "        print(f\"Processing {filename}...\")\n",
    "        try:\n",
    "            if os.path.isdir(file_path):\n",
    "                continue\n",
    "\n",
    "            book_info, ext = os.path.splitext(filename)\n",
    "            author, title, year = get_book_info(book_info)\n",
    "\n",
    "            if ext.lower() == \".txt\":\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    text = file.read()\n",
    "                pages = split_pages(text, min_words=min_words)\n",
    "\n",
    "                compiled_books[title] = {\n",
    "                    \"author\": author,\n",
    "                    \"year\": year,\n",
    "                    \"pages\": pages\n",
    "                }\n",
    "\n",
    "            elif ext.lower() == \".pdf\":\n",
    "                pages, empty, garbage, glued = extract_pdf_text(file_path)\n",
    "                if empty:\n",
    "                    print(f\"Empty text detected in {filename}.\")\n",
    "                    ocr_pile.append(file_path)\n",
    "                elif garbage:\n",
    "                    print(f\"Garbled text detected in {filename}.\")\n",
    "                    ocr_pile.append(file_path)\n",
    "                elif glued:\n",
    "                    print(f\"Glued text detected in {filename}.\")\n",
    "                    ocr_pile.append(file_path)\n",
    "                else:\n",
    "                    compiled_books[title] = {\n",
    "                        \"author\": author,\n",
    "                        \"year\": year,\n",
    "                        \"pages\": pages\n",
    "                    }\n",
    "\n",
    "            else:\n",
    "                print(f\"Skipping unsupported file type: {filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            error_pile.append(file_path)\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(compiled_books, json_file, indent=2, ensure_ascii=False)\n",
    "\n",
    "    return compiled_books, ocr_pile, error_pile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_dict, ocr_pile, error_pile = create_books_dict(\n",
    "    folder_path=\"/mnt/c/Users/jeanl/OneDrive/FUSE/data/livres\",\n",
    "    output_json_path=\"/home/jeanluca/code/JeanLucaSchindler/FUSE/FUSE_Module0a/Preproc/test.json\",\n",
    "    min_words=400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90, 31, 2)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(books_dict), len(ocr_pile), len(error_pile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1170, 403, 26)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# estimating how many books in each pile\n",
    "len(books_dict)*13, len(ocr_pile)*13, len(error_pile)*13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normal preproc will run for 5 hours\n",
      "OCR will run for 27 hours\n"
     ]
    }
   ],
   "source": [
    "OCR_min_per_book = 4\n",
    "normal_min_per_book = 0.25\n",
    "\n",
    "print(f'Normal preproc will run for {round(normal_min_per_book*len(books_dict)*13/60)} hours')\n",
    "\n",
    "print(f'OCR will run for {round(OCR_min_per_book*len(ocr_pile)*13/60)} hours')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
