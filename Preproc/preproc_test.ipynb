{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PyPDF2 import PdfReader\n",
    "import fitz\n",
    "import pdfplumber\n",
    "from pdf2image import convert_from_path\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(string):\n",
    "    cleaned_s = re.sub(r'[\\x00-\\x1F\\x7F]', ' ', string)\n",
    "    return cleaned_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_garbled(text):\n",
    "    \"\"\"Detects garbled text output from PDFs.\"\"\"\n",
    "    # Check for (cid:xxx) patterns\n",
    "    if re.search(r\"\\(cid:\\d+\\)\", text):\n",
    "        return True\n",
    "\n",
    "    # Check for excessive non-ASCII characters\n",
    "    non_ascii_ratio = sum(1 for char in text if ord(char) > 127) / max(len(text), 1)\n",
    "    if non_ascii_ratio > 0.3:  # Adjust threshold as needed\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glued_text(text, long_word_threshold=9, glue_ratio_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Detects if a text has excessive glued words (missing spaces).\n",
    "\n",
    "    Args:\n",
    "        text (str): The extracted text.\n",
    "        long_word_threshold (int): Word length above which words are considered suspicious.\n",
    "        glue_ratio_threshold (float): Threshold ratio of suspicious words for flagging text.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if text likely contains glued words, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    words = text.split()\n",
    "    total_words = len(words)\n",
    "\n",
    "    # If there's no text or no words, we can't meaningfully flag it\n",
    "    if total_words == 0:\n",
    "        return False\n",
    "\n",
    "    suspicious_words = []\n",
    "    for w in words:\n",
    "        # 1) Very long words\n",
    "        if len(w) >= long_word_threshold:\n",
    "            suspicious_words.append(w)\n",
    "            continue\n",
    "\n",
    "        # 2) Lowercase-to-Uppercase transition inside the word\n",
    "        if re.search(r'[a-z][A-Z]', w):\n",
    "            suspicious_words.append(w)\n",
    "            continue\n",
    "\n",
    "        # 3) Punctuation immediately followed by a letter (no space)\n",
    "        if re.search(r'[.,;:!?][A-Za-z]', w):\n",
    "            suspicious_words.append(w)\n",
    "            continue\n",
    "\n",
    "    # Ratio of suspicious words to total words\n",
    "    suspicious_ratio = len(suspicious_words) / total_words\n",
    "\n",
    "    return suspicious_ratio > glue_ratio_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_spaced_out(line: str, min_ratio=0.5) -> bool:\n",
    "    \"\"\"\n",
    "    Returns True if the line looks like it's \"spaced-out\" text, e.g. 'T a b l e  o f  C o n t e n t s'\n",
    "    :param line: The text line to examine\n",
    "    :param min_ratio: If single-letter tokens exceed this fraction of total tokens,\n",
    "                      we treat it as spaced-out text.\n",
    "    \"\"\"\n",
    "    tokens = line.split()\n",
    "    if not tokens:\n",
    "        return False\n",
    "\n",
    "    # Count how many tokens are single letters\n",
    "    single_letter_count = sum(1 for tok in tokens if len(tok) == 1)\n",
    "    ratio = single_letter_count / len(tokens)\n",
    "    return ratio >= min_ratio\n",
    "\n",
    "def fix_spaced_line(line: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts spaced-out sequences like 'T a b l e' into 'Table',\n",
    "    but treats any sequence of >=2 spaces as a boundary between words.\n",
    "    Example:\n",
    "      \"T a b l e  o f  C o n t e n t s  P E N G U I N  C L A S S I C S\"\n",
    "      becomes \"Table of Contents PENGUIN CLASSICS\"\n",
    "    \"\"\"\n",
    "    # 1) Split the line on double-or-more spaces, capturing each chunk\n",
    "    #    For instance, with \"T a b l e  o f  C o n t e n t s  P E N G U I N\"\n",
    "    #    we get segments [\"T a b l e\", \"o f\", \"C o n t e n t s\", \"P E N G U I N\", \"C L A S S I C S\"]\n",
    "    segments = re.split(r\"\\s{2,}\", line.strip())\n",
    "\n",
    "    fixed_words = []\n",
    "    for seg in segments:\n",
    "        # 2) Within each segment, we might have single-letter tokens\n",
    "        #    separated by single spaces. So we gather them and form one word.\n",
    "        #    \"T a b l e\" => [\"T\",\"a\",\"b\",\"l\",\"e\"] => \"Table\"\n",
    "        #    BUT if the segment already has multi-letter tokens, we keep them as is.\n",
    "\n",
    "        tokens = seg.split()\n",
    "\n",
    "        # Check if they're all single letters. If yes, merge them into one word.\n",
    "        if all(len(t) == 1 for t in tokens):\n",
    "            merged = \"\".join(tokens)  # e.g. \"T a b l e\" => \"Table\"\n",
    "            fixed_words.append(merged)\n",
    "        else:\n",
    "            # If there's a mix (or multi-letter tokens),\n",
    "            # we can do a more nuanced approach:\n",
    "            #   - accumulate consecutive single letters,\n",
    "            #   - keep multi-letter tokens as is.\n",
    "            new_sublist = []\n",
    "            temp_cluster = []\n",
    "\n",
    "            for t in tokens:\n",
    "                if len(t) == 1:\n",
    "                    temp_cluster.append(t)\n",
    "                else:\n",
    "                    # if we have a cluster accumulated, join them first\n",
    "                    if temp_cluster:\n",
    "                        new_sublist.append(\"\".join(temp_cluster))\n",
    "                        temp_cluster = []\n",
    "                    new_sublist.append(t)\n",
    "\n",
    "            # If ended with single-letter cluster\n",
    "            if temp_cluster:\n",
    "                new_sublist.append(\"\".join(temp_cluster))\n",
    "\n",
    "            # Now new_sublist might be e.g. [\"This\", \"Table\", \"has\", \"MultiWord\", \"stuff\"]\n",
    "            # We'll rejoin with a single space because these are separate words.\n",
    "            fixed_words.append(\" \".join(new_sublist))\n",
    "\n",
    "    # 3) Finally, rejoin *segments* with a single space.\n",
    "    #    So we get e.g. \"Table of Contents PENGUIN CLASSICS\".\n",
    "    return \" \".join(fixed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_pages(text, min_words=400):\n",
    "    \"\"\"Splits text into pages at paragraph breaks if they exceed min_words.\"\"\"\n",
    "    pages = []\n",
    "    current_page_lines = []\n",
    "    current_word_count = 0\n",
    "    threshold_reached = False\n",
    "\n",
    "    for line in text.splitlines():\n",
    "        current_page_lines.append(line)\n",
    "        current_word_count += len(line.split())\n",
    "\n",
    "        if current_word_count >= min_words:\n",
    "            threshold_reached = True\n",
    "\n",
    "        if threshold_reached and not line.strip():\n",
    "            page_text = \" \".join(current_page_lines).strip()\n",
    "            page_text = clean_text(page_text)\n",
    "            if page_text:\n",
    "                pages.append(page_text)\n",
    "            current_page_lines = []\n",
    "            current_word_count = 0\n",
    "            threshold_reached = False\n",
    "\n",
    "    if current_page_lines:\n",
    "        page_text = \"\".join(current_page_lines).strip()\n",
    "        if page_text:\n",
    "            pages.append(page_text)\n",
    "\n",
    "    return pages\n",
    "\n",
    "def extract_pdf_text(file_path):\n",
    "    \"\"\"Extracts text from a PDF page-by-page.\"\"\"\n",
    "\n",
    "    text_pages = []\n",
    "    glued_count = 0\n",
    "    garbage_count = 0\n",
    "\n",
    "    doc = fitz.open(file_path)\n",
    "    num_pages = len(doc)  # Get the total number of pages\n",
    "    all_text_empty = True\n",
    "    garbage = False\n",
    "    glued = False\n",
    "\n",
    "    for page_num in range(num_pages):  # FIX: Use range(num_pages)\n",
    "        page = doc[page_num]\n",
    "        text = page.get_text(\"text\")\n",
    "\n",
    "        if not text.strip():\n",
    "            text_pages.append(\"\")  # Ensure page count matches index\n",
    "            continue\n",
    "\n",
    "        # Clean text\n",
    "        text = clean_text(text)\n",
    "        if is_spaced_out(text):\n",
    "            text = fix_spaced_line(text)\n",
    "\n",
    "        # Check if problematic text\n",
    "        glued = glued_text(text)\n",
    "        garbage = is_garbled(text)\n",
    "\n",
    "        if glued:\n",
    "            glued_count += 1\n",
    "        else:\n",
    "            glued_count = 0\n",
    "\n",
    "        if garbage:\n",
    "            garbage_count += 1\n",
    "        else:\n",
    "            garbage_count = 0\n",
    "\n",
    "        if glued or garbage:\n",
    "            text_pages.append(\"\")\n",
    "        else:\n",
    "            text_pages.append(text.strip())\n",
    "            all_text_empty = False\n",
    "\n",
    "        # Stop processing if too many bad pages\n",
    "        if glued_count >= 5 or garbage_count >= 5:\n",
    "            break\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "    return text_pages, all_text_empty, (garbage_count > 0), (glued_count > 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_info(filename):\n",
    "    try:\n",
    "        filename_list = filename.split('_')\n",
    "        author = filename_list[0]\n",
    "        title = filename_list[1]\n",
    "        year = filename_list[2]\n",
    "\n",
    "        return author, title, year\n",
    "    except:\n",
    "        return \"unknown\", filename, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_books_dict(folder_path, output_json_path, min_words=400):\n",
    "    \"\"\"\n",
    "    Processes a folder of files and splits them into pages, handling:\n",
    "      - .txt files with paragraph splitting\n",
    "      - .pdf files (detecting digital text or applying OCR for scanned/printed PDFs)\n",
    "      - Skips unsupported files\n",
    "\n",
    "    Outputs a dictionary:\n",
    "    {\n",
    "        \"filename\": {\n",
    "            \"author\": \"default_author\",\n",
    "            \"pages\": [...]\n",
    "        },\n",
    "        ...\n",
    "    }\n",
    "\n",
    "    Also saves this dictionary to a JSON file.\n",
    "    \"\"\"\n",
    "    compiled_books = {}\n",
    "\n",
    "    ocr_pile = []\n",
    "    error_pile = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        print()\n",
    "        print(f\"Processing {filename}...\")\n",
    "        try:\n",
    "            if os.path.isdir(file_path):\n",
    "                continue\n",
    "\n",
    "            book_info, ext = os.path.splitext(filename)\n",
    "            author, title, year = get_book_info(book_info)\n",
    "\n",
    "            if ext.lower() == \".txt\":\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    text = file.read()\n",
    "                pages = split_pages(text, min_words=min_words)\n",
    "\n",
    "                compiled_books[book_info] = {\n",
    "                    \"title\": title,\n",
    "                    \"author\": author,\n",
    "                    \"year\": year,\n",
    "                    \"pages\": pages\n",
    "                }\n",
    "\n",
    "\n",
    "            elif ext.lower() == \".pdf\":\n",
    "                pages, empty, garbage, glued = extract_pdf_text(file_path)\n",
    "                if garbage:\n",
    "                    print(f\"Garbled text detected in {filename}.\")\n",
    "                    ocr_pile.append(file_path)\n",
    "                elif glued:\n",
    "                    print(f\"Glued text detected in {filename}.\")\n",
    "                    ocr_pile.append(file_path)\n",
    "                elif empty:\n",
    "                    print(f\"Empty text detected in {filename}.\")\n",
    "                    ocr_pile.append(file_path)\n",
    "                else:\n",
    "                    compiled_books[book_info] = {\n",
    "                        \"title\": title,\n",
    "                        \"author\": author,\n",
    "                        \"year\": year,\n",
    "                        \"pages\": pages\n",
    "                    }\n",
    "\n",
    "\n",
    "            else:\n",
    "                print(f\"Skipping unsupported file type: {filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            error_pile.append(file_path)\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(compiled_books, json_file, indent=2, ensure_ascii=False)\n",
    "\n",
    "    return compiled_books, ocr_pile, error_pile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Pierre Bourdieu_Distinction, A Social Critique of the Judgement of Taste_1979 (1).pdf...\n",
      "Garbled text detected in Pierre Bourdieu_Distinction, A Social Critique of the Judgement of Taste_1979 (1).pdf.\n",
      "\n",
      "Processing Marianne Moore_Complete Poems_1967.pdf...\n",
      "\n",
      "Processing Iris Murdoch_The Sea the Sea_1978.pdf...\n",
      "\n",
      "Processing Xenophon_Anabasis_370 BC.txt...\n",
      "\n",
      "Processing Mary Wollstonecraft_A Vindication of the Rights of Women_1792.pdf...\n",
      "\n",
      "Processing Henrik Ibsen_Hedda Gabler_1890.txt...\n",
      "\n",
      "Processing Yukio Mishima_The Sailor Who Fell From Grace with the Sea_1963.pdf...\n"
     ]
    }
   ],
   "source": [
    "books_dict, ocr_pile, error_pile = create_books_dict(\n",
    "    folder_path=\"data\",\n",
    "    output_json_path=\"/home/jeanluca/code/JeanLucaSchindler/FUSE/FUSE_Module0a/Preproc/test.json\",\n",
    "    min_words=400\n",
    ")\n",
    "\n",
    "# time for 100 books: 12m 30s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 1, 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(books_dict), len(ocr_pile), len(error_pile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A NOTE ON THE TEXT The text conforms as closely as is now possible to the author’s final intentions. Five of the poems written after the first printing of this volume have been included. Late authorized corrections, and earlier corrections authorized but not made, have been incorporated. Punctuation, hyphens, and line arrangements silently changed by editor, proofreader, or typesetter have been restored. Misleading editorial amplifications of the notes have been removed. Clive Driver OceanofPDF.com'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 13\n",
    "books_dict[\"Marianne Moore_Complete Poems_1967\"]['pages'][i-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjust json to mongodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_json_to_mongodb(dict_style_json, replacement):\n",
    "    \"\"\"\n",
    "    Converts dict-style JSON to list-style for MongoDB compatibility,\n",
    "    and replaces empty strings in 'pages' with a single space.\n",
    "\n",
    "    Parameters:\n",
    "        dict_style_json (dict): Dictionary-style JSON (keys as filenames or book IDs)\n",
    "\n",
    "    Returns:\n",
    "        list: List-style JSON suitable for MongoDB insertion\n",
    "    \"\"\"\n",
    "    mongo_ready = []\n",
    "\n",
    "    for book_id, book_data in dict_style_json.items():\n",
    "        # Copy to avoid mutating original\n",
    "        book = book_data.copy()\n",
    "\n",
    "        # Ensure 'title' exists (if you want to keep the original dict key, you can add it as a field)\n",
    "        if \"title\" not in book:\n",
    "            book[\"title\"] = book_id\n",
    "\n",
    "        # Sanitize pages\n",
    "        if \"pages\" in book and isinstance(book[\"pages\"], list):\n",
    "            book[\"pages\"] = [p if p.strip() else replacement for p in book[\"pages\"]]\n",
    "\n",
    "        mongo_ready.append(book)\n",
    "\n",
    "    return mongo_ready\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "book_list = adjust_json_to_mongodb(books_dict, None)\n",
    "\n",
    "with open(\"test_list.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(book_list, f, indent=2, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
