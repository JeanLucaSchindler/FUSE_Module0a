{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PyPDF2 import PdfReader\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(string):\n",
    "    cleaned_s = re.sub(r'[\\x00-\\x1F\\x7F]', '', string)\n",
    "    return cleaned_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_garbled(text):\n",
    "    \"\"\"Detects garbled text output from PDFs.\"\"\"\n",
    "    # Check for (cid:xxx) patterns\n",
    "    if re.search(r\"\\(cid:\\d+\\)\", text):\n",
    "        return True\n",
    "\n",
    "    # Check for excessive non-ASCII characters\n",
    "    non_ascii_ratio = sum(1 for char in text if ord(char) > 127) / max(len(text), 1)\n",
    "    if non_ascii_ratio > 0.3:  # Adjust threshold as needed\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glued_text(text, long_word_threshold=30, glue_ratio_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Detects if a text has excessive glued words (missing spaces).\n",
    "\n",
    "    Args:\n",
    "        text (str): The extracted text.\n",
    "        long_word_threshold (int): Word length above which words are considered suspicious.\n",
    "        glue_ratio_threshold (float): Threshold ratio of glued words for flagging text.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if text likely contains glued words, False otherwise.\n",
    "    \"\"\"\n",
    "\n",
    "    words = text.split()\n",
    "    total_words = len(words)\n",
    "\n",
    "    if total_words == 0:\n",
    "        return False  # Empty text is not glued\n",
    "\n",
    "    # Count long words\n",
    "    long_words = [w for w in words if len(w) >= long_word_threshold]\n",
    "\n",
    "    # Count capitalized words glued together (e.g., \"TokyoImperialUniversity\")\n",
    "    glued_caps_words = [w for w in words if re.search(r'[a-z][A-Z]', w)]\n",
    "\n",
    "    # Calculate glue ratio\n",
    "    glue_ratio = len(long_words) / total_words\n",
    "    caps_glue_ratio = len(glued_caps_words) / total_words\n",
    "\n",
    "    # If too many long words or glued capital words, flag as glued text\n",
    "    if glue_ratio > glue_ratio_threshold or caps_glue_ratio > glue_ratio_threshold:\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_pages(text, min_words=400):\n",
    "    \"\"\"Splits text into pages at paragraph breaks if they exceed min_words.\"\"\"\n",
    "    pages = []\n",
    "    current_page_lines = []\n",
    "    current_word_count = 0\n",
    "    threshold_reached = False\n",
    "\n",
    "    for line in text.splitlines():\n",
    "        current_page_lines.append(line)\n",
    "        current_word_count += len(line.split())\n",
    "\n",
    "        if current_word_count >= min_words:\n",
    "            threshold_reached = True\n",
    "\n",
    "        if threshold_reached and not line.strip():\n",
    "            page_text = \" \".join(current_page_lines).strip()\n",
    "            page_text = clean_text(page_text)\n",
    "            if page_text:\n",
    "                pages.append(page_text)\n",
    "            current_page_lines = []\n",
    "            current_word_count = 0\n",
    "            threshold_reached = False\n",
    "\n",
    "    if current_page_lines:\n",
    "        page_text = \"\".join(current_page_lines).strip()\n",
    "        if page_text:\n",
    "            pages.append(page_text)\n",
    "\n",
    "    return pages\n",
    "\n",
    "def extract_pdf_text(file_path):\n",
    "    \"\"\"Extracts text from a PDF page-by-page.\"\"\"\n",
    "\n",
    "    text_pages = []\n",
    "\n",
    "    with open(file_path, \"rb\") as file:\n",
    "        reader = PdfReader(file)\n",
    "        all_text_empty = True\n",
    "        garbage = False\n",
    "        glued = False\n",
    "\n",
    "        for page in reader.pages:\n",
    "            text = page.extract_text()\n",
    "            text = clean_text(text)\n",
    "            glued =  glued_text(text)\n",
    "            garbage = is_garbled(text)\n",
    "            if garbage or glued:\n",
    "                break\n",
    "            if text and text.strip():\n",
    "                all_text_empty = False\n",
    "                text_pages.append(text.strip())\n",
    "            else:\n",
    "                text_pages.append(\"\")  # placeholder for now\n",
    "\n",
    "    return text_pages, all_text_empty, garbage, glued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book_info(filename):\n",
    "    try:\n",
    "        filename_list = filename.split('_')\n",
    "        author = filename_list[0]\n",
    "        title = filename_list[1]\n",
    "        year = filename_list[2]\n",
    "\n",
    "        return author, title, year\n",
    "    except:\n",
    "        return \"unknown\", filename, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_books_dict(folder_path, output_json_path, min_words=400):\n",
    "    \"\"\"\n",
    "    Processes a folder of files and splits them into pages, handling:\n",
    "      - .txt files with paragraph splitting\n",
    "      - .pdf files (detecting digital text or applying OCR for scanned/printed PDFs)\n",
    "      - Skips unsupported files\n",
    "\n",
    "    Outputs a dictionary:\n",
    "    {\n",
    "        \"filename\": {\n",
    "            \"author\": \"default_author\",\n",
    "            \"pages\": [...]\n",
    "        },\n",
    "        ...\n",
    "    }\n",
    "\n",
    "    Also saves this dictionary to a JSON file.\n",
    "    \"\"\"\n",
    "    compiled_books = {}\n",
    "\n",
    "    ocr_pile = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        print()\n",
    "        print(f\"Processing {filename}...\")\n",
    "\n",
    "        if os.path.isdir(file_path):\n",
    "            continue\n",
    "\n",
    "        book_info, ext = os.path.splitext(filename)\n",
    "        author, title, year = get_book_info(book_info)\n",
    "\n",
    "        if ext.lower() == \".txt\":\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "            pages = split_pages(text, min_words=min_words)\n",
    "\n",
    "            compiled_books[title] = {\n",
    "                \"author\": author,\n",
    "                \"year\": year,\n",
    "                \"pages\": pages\n",
    "            }\n",
    "\n",
    "        elif ext.lower() == \".pdf\":\n",
    "            pages, empty, garbage, glued = extract_pdf_text(file_path)\n",
    "            if empty:\n",
    "                print(f\"Empty text detected in {filename}.\")\n",
    "                ocr_pile.append(file_path)\n",
    "            elif garbage:\n",
    "                print(f\"Garbled text detected in {filename}.\")\n",
    "                ocr_pile.append(file_path)\n",
    "            elif glued:\n",
    "                print(f\"Glued text detected in {filename}.\")\n",
    "                ocr_pile.append(file_path)\n",
    "            else:\n",
    "                compiled_books[title] = {\n",
    "                    \"author\": author,\n",
    "                    \"year\": year,\n",
    "                    \"pages\": pages\n",
    "                }\n",
    "\n",
    "        else:\n",
    "            print(f\"Skipping unsupported file type: {filename}\")\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(compiled_books, json_file, indent=2, ensure_ascii=False)\n",
    "\n",
    "    return compiled_books, ocr_pile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Claude Levi-Strauss_Tristes_Tropiques_1955.pdf...\n",
      "\n",
      "Processing Pierre Bourdieu_Distinction, A Social Critique of the Judgement of Taste_1979 (1).pdf...\n",
      "Garbled text detected in Pierre Bourdieu_Distinction, A Social Critique of the Judgement of Taste_1979 (1).pdf.\n",
      "\n",
      "Processing Marianne Moore_Complete Poems_1967.pdf...\n",
      "\n",
      "Processing Xenophon_Anabasis_370 BC.txt...\n",
      "\n",
      "Processing Mary Wollstonecraft_A Vindication of the Rights of Women_1792.pdf...\n",
      "\n",
      "Processing Accenture CL.pdf...\n",
      "Empty text detected in Accenture CL.pdf.\n",
      "\n",
      "Processing Henrik Ibsen_Hedda Gabler_1890.txt...\n",
      "\n",
      "Processing Yukio Mishima_The Sailor Who Fell From Grace with the Sea_1963.pdf...\n",
      "Glued text detected in Yukio Mishima_The Sailor Who Fell From Grace with the Sea_1963.pdf.\n"
     ]
    }
   ],
   "source": [
    "books_dict, ocr_pile = create_books_dict(\n",
    "    folder_path=\"/home/jeanluca/code/JeanLucaSchindler/FUSE/FUSE_Module0a/Preproc/data\",\n",
    "    output_json_path=\"/home/jeanluca/code/JeanLucaSchindler/FUSE/FUSE_Module0a/Preproc/test.json\",\n",
    "    min_words=400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISHIMA. (6)e Sailor Who Fell from Grace with the SeaNow a disturbing and erotic film starring Sarah Miles and Kris Kristofferson. aeye vs < 7/ Seayh vy ‘ -— . aee My s a————,\" - 3\n"
     ]
    }
   ],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import concurrent.futures\n",
    "\n",
    "def ocr_page(image, lang=\"eng\"):\n",
    "    \"\"\"Performs OCR on a single image page.\"\"\"\n",
    "    return pytesseract.image_to_string(image, lang=lang, config=\"--psm 6\")  # PSM 6 is optimized for dense text\n",
    "\n",
    "def fast_ocr_pdf_to_text_list(pdf_path, dpi=200, lang=\"eng\", workers=4):\n",
    "    \"\"\"\n",
    "    Efficiently reads a scanned PDF using OCR and returns a list where each element is a string representing a PDF page.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "        dpi (int): Lower DPI speeds up processing but may reduce accuracy.\n",
    "        lang (str): OCR language (default: \"eng\" for English).\n",
    "        workers (int): Number of parallel threads for processing.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list where each element is the text of a corresponding PDF page.\n",
    "    \"\"\"\n",
    "    # Convert PDF pages to images\n",
    "    images = convert_from_path(pdf_path, dpi=dpi)\n",
    "\n",
    "    # Run OCR in parallel using ThreadPoolExecutor\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=workers) as executor:\n",
    "        text_per_page = list(executor.map(lambda img: ocr_page(img, lang), images))\n",
    "\n",
    "    return text_per_page\n",
    "\n",
    "res = fast_ocr_pdf_to_text_list(ocr_pile[2])\n",
    "# ran for 4min 20s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summerimitation damask of the couch gave the room an air ofagitation. She must have noticed a ladder on her way outand changed in a hurry.Only dazzling sky and a few fragments of cloud, hardand glossy as enamel in the light bouncing off the water,could be seen through the window.Noboru couldn’t believe he was looking at his mother’sbedroom; it might have belonged to a stranger. But therewas no doubt that a woman lived there: femininitytrembled in every corner, a faint scent lingered in theair.Then a strange idea assailed him. Did the peephole justhappen to be here, an accident? Or — after the war —when the soldiers’ families had been living together in thehouse . . . He had a sudden feeling that another body,larger than his, a blond, hairy body, had once huddled inthis dusty space in the wall. The thought soured the closeair and he was sickened. Wriggling backwards out of thechest, he ran to the next room. He would never forget thequeer sensation he had when, flinging open the door, heburst in.Drab and familiar, the room bore no resemblance tothe mysterious chamber he had seen through the peep-hole: it was here that he came to whine and to sulk — it’stime you stopped coming into Mother's room so oftenwith that excuse about wanting to watch the ships; you'renot a child any more, dear — here that his mother wouldput aside her embroidery to help him with his homeworkwhile she stifled yawns, or would scold him for not tyinghis tie straight, or would check the ledgers she broughthome from the shop...He looked for the peephole. It wasn’t easy to find.Cunningly hidden in the ornately carved wainscot, in a7\n"
     ]
    }
   ],
   "source": [
    "print(clean_text(res[6]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lewagon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
